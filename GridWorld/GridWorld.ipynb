{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic GridWorld home-made example, based on Sutton and Barto's \"Reinforcement Learning\" book example p.77\n",
    "\n",
    "The GridWorld is a 2D world of NX x NY squares.\n",
    "The goal is to go from any square in the world to the upper-left or lower-right corner of the world, as fast as possible\n",
    "\n",
    "A state is the position of a square, with coordinates (x,y) (0<= x <=NX-1, 0 <= y <= NY-1). (0,0) is the upper left corner.\n",
    "\n",
    "An action is one of the following four : up, down, right or left. A state does not change if an action attempts to go outside the grid (for example \"up\" when y=0)\n",
    "\n",
    "The MDP dynamcis are DETERMINISTIC in this example : an action will move a state to another state (possibly the same) with probability one.abs\n",
    "\n",
    "A policy $\\pi$ associates each state with four probabilities (summing up to one) of going up,down,right, left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- librairies -----------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- hyperparameters ------------------------------------------------------------------------\n",
    "\n",
    "# world size\n",
    "NX = 4\n",
    "NY = 4\n",
    "# discount\n",
    "GAMMA = 1.0\n",
    "# actions\n",
    "actions = {\n",
    "    0 : (0,-1), # up\n",
    "    1 : (0,+1), # down\n",
    "    2 : (+1,0), # right\n",
    "    3 : (-1,0), # left\n",
    "}\n",
    "NUM_ACTIONS = len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Value Function -------\n",
    "\n",
    "class ValueFunction():\n",
    "    \"\"\"Value function class. Stores value functions for each state, provides basic get, update and display methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nx=NX, ny=NY, value_function=None):\n",
    "        assert 0<nx and 0<ny and isinstance(nx,int) and isinstance(ny, int), f\"Erreur paramètres constructeur ValueFunction\"\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        if value_function is None:\n",
    "            self.vf = np.zeros(shape=(self.nx, self.ny))\n",
    "        else:\n",
    "            self.vf = value_function.vf\n",
    "            \n",
    "    def get(self, x,y):\n",
    "        assert (0 <= x < self.nx) and (0 <= y < self.ny), \"erreur : hors grid dans ValueFunction.get()\"\n",
    "        return self.vf[x,y]\n",
    "    \n",
    "    def update(self, x,y, value):\n",
    "        assert (0 <= x < self.nx) and (0 <= y < self.ny), \"erreur : hors grid dans ValueFunction.update()\"\n",
    "        self.vf[x,y] = value\n",
    "        \n",
    "    def display(self):\n",
    "        print(self.vf)\n",
    "\n",
    "    def __repr__(self):\n",
    "        msg = f\"Objet ValueFunction taille {self.nx} x {self.ny}\"\n",
    "        return msg\n",
    "        \n",
    "    def __str__(self):\n",
    "        msg = f\"Objet ValueFunction taille {self.nx} x {self.ny}\"\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Policy -----------------\n",
    "\n",
    "class Policy():\n",
    "    \"\"\"Policy class. Stores probabilities of each action (up, down, right, left) per state.\n",
    "    \"\"\"\n",
    "    action_to_str = {\n",
    "        0 : \"U\",\n",
    "        1 : \"D\",\n",
    "        2 : \"R\",\n",
    "        3 : \"L\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, nx=NX, ny=NY, policy=None):\n",
    "        assert 0<nx and 0<ny and isinstance(nx,int) and isinstance(ny, int), f\"Erreur paramètres constructeur Policy\"\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        if policy is None:\n",
    "            self.policy = np.full(shape=(self.nx, self.ny, NUM_ACTIONS), fill_value=1/NUM_ACTIONS)  # default is equiprobable random policy\n",
    "        else:\n",
    "            self.policy = policy\n",
    "            \n",
    "    def get(self, x,y):\n",
    "        assert (0 <= x < self.nx) and (0 <= y < self.ny), \"erreur : hors grid dans Policy.get()\"\n",
    "        return self.policy[x,y]\n",
    "    \n",
    "    def update(self, x,y, value):\n",
    "        # value is a np.array shape NUM_ACTIONS x 1\n",
    "        assert (0 <= x < self.nx) and (0 <= y < self.ny), \"erreur : hors grid dans Policy.update()\"\n",
    "        self.policy[x,y] = value\n",
    "        \n",
    "    def display(self):\n",
    "        print(self.policy)\n",
    "        \n",
    "    def get_graphic_display(self):\n",
    "        chars = np.full(shape=(self.nx, self.ny), dtype=object, fill_value=\"\")\n",
    "        for x in range(self.nx):\n",
    "            for y in range(self.ny):\n",
    "                local = self.get(x,y) # get local policy\n",
    "                msg = \"\"\n",
    "                for action_number in actions.keys():\n",
    "                    value = local[action_number]\n",
    "                    msg = msg + self.action_to_str.get(action_number) + f\"({value:.2f})\"\n",
    "                chars[x,y] = msg\n",
    "        return np.transpose(chars)  # transpose because of the x,y coordinates convention \n",
    "\n",
    "    def __repr__(self):\n",
    "        msg = f\"Objet Policy taille {self.nx} x {self.ny} x {NUM_ACTIONS} - shape = {self.policy.shape}\"\n",
    "        return msg\n",
    "        \n",
    "    def __str__(self):\n",
    "        msg = f\"Objet Policy taille {self.nx} x {self.ny} x {NUM_ACTIONS} - shape = {self.policy.shape}\"\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- MDP Dynamics ----------------\n",
    "\n",
    "class MDPDynamics():\n",
    "    \"\"\"Code the dynamics of the MDP. \n",
    "    For GridWorld, this is a deterministic dynamic : the next state is reached with probability one\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, actions=actions):\n",
    "        self.actions = actions\n",
    "        \n",
    "    def step(self, x,y, action_number):\n",
    "        \"\"\"calculate next step\n",
    "\n",
    "        Args:\n",
    "            x, y (ints) : coordinates of the current state\n",
    "            action_number (int): code of the action\n",
    "            \n",
    "        Returns :\n",
    "            x_new, y_new (ints): coordinates of the state being reached\n",
    "            reward (int): reward associated to the move\n",
    "            end (boolean) : True if terminal state is reached\n",
    "        \"\"\"\n",
    "        step_x, step_y = actions.get(action_number)\n",
    "        \n",
    "        x_new = x + step_x\n",
    "        if x_new < 0: x_new = 0\n",
    "        if x_new >= NX: x_new = NX-1\n",
    "        \n",
    "        y_new = y + step_y\n",
    "        if y_new < 0: y_new = 0\n",
    "        if y_new >= NY: y_new = NY-1\n",
    "        \n",
    "        if (x_new, y_new)==(0,0) or (x_new,y_new)==(NX-1,NY-1):\n",
    "            end = True\n",
    "            reward = -1\n",
    "        else:\n",
    "            end = False\n",
    "            reward = -1\n",
    "        \n",
    "        return x_new, y_new, reward, end\n",
    "    \n",
    "    def __repr__(self):\n",
    "        msg = f\"Objet MDPDynamics. Actions = {self.actions}\"\n",
    "        return msg\n",
    "    \n",
    "    def __str__(self):\n",
    "        msg = f\"Objet MDPDynamics. Actions = {self.actions}\"\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# --- Policy Evaluation --------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class IterativePolicyEvaluation():\n",
    "    \"\"\"Calculate one iteration step of a value function towards the policy's value function v_pi\n",
    "    \"\"\"\n",
    "    # THETA = 1e-6 # threshold to stop iterating\n",
    "    dynamics = MDPDynamics()\n",
    "    \n",
    "    def __init__(self, policy, vf_old=None):\n",
    "        \"\"\"Constructor, to evaluate a given <policy> iteratively starting from a ValueFunction <v_start>\n",
    "\n",
    "        Args:\n",
    "            policy (Policy): object Policy to evaluate.\n",
    "            v_start (ValueFunction, optional): ValueFunction to use as a start of the algorithm. Defaults to None, in which case 0 is used\n",
    "        \"\"\"\n",
    "        # policy to evaluate\n",
    "        self.policy = policy\n",
    "        # value function to use as a first iteration\n",
    "        if vf_old is None:\n",
    "            self.vf_old = ValueFunction()  # the default ValueFunction is 0 for all states\n",
    "        else:\n",
    "            self.vf_old = vf_old\n",
    "        # store first iteration for record\n",
    "        self.vf_start = self.vf_old\n",
    "        # value function calculation for policy, place holder\n",
    "        self.vf_new = ValueFunction()\n",
    "            \n",
    "    def evaluation_step(self):\n",
    "        \"\"\"Return one step evaluation of the policy\"\"\"\n",
    "        for x in range(NX):\n",
    "            for y in range(NY):\n",
    "                # state s is (x,y)\n",
    "                if (x,y) != (0,0) and (x,y) != (NX-1,NY-1):  # update value function for non terminal states only\n",
    "                    for action_number in actions.keys():\n",
    "                        # action is action_number\n",
    "                        # get s' and r\n",
    "                        x_new, y_new, reward, end = self.dynamics.step(x,y,action_number)\n",
    "                        # update vf_new(x,y)\n",
    "                        self.vf_new.vf[x,y] += self.policy.get(x,y)[action_number] * (reward + GAMMA * self.vf_old.vf[x_new, y_new])\n",
    "        # calculate Norm 2 between the update and the original\n",
    "        delta_vf = np.linalg.norm(self.vf_new.vf - self.vf_old.vf)\n",
    "        \n",
    "        return self.vf_new, delta_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function avant itération :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Iteration 100\n",
      "Value Function après calcul :\n",
      "[[  0.         -13.94260509 -19.91495107 -21.90482522]\n",
      " [-13.94260509 -17.92507693 -19.91551999 -19.91495107]\n",
      " [-19.91495107 -19.91551999 -17.92507693 -13.94260509]\n",
      " [-21.90482522 -19.91495107 -13.94260509   0.        ]]\n",
      "Norme 2 = 0.0164505\n",
      "Iteration 200\n",
      "Value Function après calcul :\n",
      "[[  0.         -13.99975741 -19.99964052 -21.99959772]\n",
      " [-13.99975741 -17.99968332 -19.99964293 -19.99964052]\n",
      " [-19.99964052 -19.99964293 -17.99968332 -13.99975741]\n",
      " [-21.99959772 -19.99964052 -13.99975741   0.        ]]\n",
      "Norme 2 = 0.0000695\n",
      "Iteration 300\n",
      "Value Function après calcul :\n",
      "[[  0.         -13.99999897 -19.99999848 -21.9999983 ]\n",
      " [-13.99999897 -17.99999866 -19.99999849 -19.99999848]\n",
      " [-19.99999848 -19.99999849 -17.99999866 -13.99999897]\n",
      " [-21.9999983  -19.99999848 -13.99999897   0.        ]]\n",
      "Norme 2 = 0.0000003\n",
      "Iteration 400\n",
      "Value Function après calcul :\n",
      "[[  0.         -14.         -19.99999999 -21.99999999]\n",
      " [-14.         -17.99999999 -19.99999999 -19.99999999]\n",
      " [-19.99999999 -19.99999999 -17.99999999 -14.        ]\n",
      " [-21.99999999 -19.99999999 -14.           0.        ]]\n",
      "Norme 2 = 0.0000000\n",
      "Iteration 500\n",
      "Value Function après calcul :\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "Norme 2 = 0.0000000\n",
      "\n",
      "\n",
      "Calcul de la value function optimale à la précision 1e-12 après 531 itérations\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------    \n",
    "# --- Calcul de la value function de la policy v_pi (sans optimisation de la policy)------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "iter_counter = 0\n",
    "THETA = 1e-12\n",
    "delta_vf = 2 * THETA\n",
    "vf_old = ValueFunction()   # instantiate a ValueFunction equal to zero for all states\n",
    "random_policy = Policy()\n",
    "ipe = IterativePolicyEvaluation(random_policy)\n",
    "\n",
    "print(f\"Value Function avant itération :\")\n",
    "vf_old.display()\n",
    "\n",
    "while delta_vf > THETA:\n",
    "    iter_counter += 1\n",
    "    vf_evaluation, delta_vf = ipe.evaluation_step()\n",
    "    if iter_counter % 100 == 0:\n",
    "        print(f\"Iteration {iter_counter}\")\n",
    "        print(f\"Value Function après calcul :\")\n",
    "        vf_evaluation.display()\n",
    "        print(f\"Norme 2 = {delta_vf:.7f}\")\n",
    "    ipe.vf_old = vf_evaluation\n",
    "    ipe.vf_new = ValueFunction()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Calcul de la value function optimale à la précision {THETA} après {iter_counter} itérations\")\n",
    "vf_evaluation.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --- Policy Improvement ---------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class PolicyImprovement():\n",
    "    \"\"\"Given a policy and a value function, return an improved policy, or signals if the policy is already optimal\n",
    "    \"\"\"\n",
    "    \n",
    "    dynamics = MDPDynamics()\n",
    "    \n",
    "    def __init__(self, policy, value_function):\n",
    "        \"\"\"instantiate the object with a given policy and a given value_function\n",
    "\n",
    "        Args:\n",
    "            policy (Policy): the starting policy, to improve\n",
    "            value_function (ValueFunction): the value function to use to improve the policy\n",
    "        \"\"\"\n",
    "        self.start_policy = policy\n",
    "        self.new_policy = Policy()\n",
    "        \n",
    "        self.start_vf = value_function\n",
    "        \n",
    "    def improvement_step(self):\n",
    "        \"\"\"logic to improve the policy. Returns optimal=True if policy already optimal\n",
    "        \"\"\"\n",
    "        \n",
    "        optimal = True\n",
    "        \n",
    "        for x in range(NX):\n",
    "            for y in range(NY):\n",
    "                # state s is (x,y)\n",
    "                if (x,y) != (0,0) and (x,y) != (NX-1,NY-1):  # update policy for non terminal states only\n",
    "                    # first, find out all four q_values for each of the four actions\n",
    "                    current_potential_q_values = np.zeros(shape=NUM_ACTIONS)\n",
    "                    for action_number in actions:\n",
    "                        # nouvel état suite à action\n",
    "                        x_new, y_new, reward, end = self.dynamics.step(x,y,action_number)\n",
    "                        # value function au nouvel état\n",
    "                        v_value = self.start_vf.get(x_new, y_new)\n",
    "                        # calcul q_value correspondante\n",
    "                        current_potential_q_values[action_number] = reward + GAMMA * v_value\n",
    "                    v_max = np.max(current_potential_q_values)\n",
    "                    idx = np.array([ 1 if current_potential_q_values[i]==v_max else 0 for i in actions.keys() ])  # 1 for action getting max value\n",
    "                    new_pol = idx / np.sum(idx) # normalize to get probabilities\n",
    "                    self.new_policy.update(x,y,new_pol)  # write new policy\n",
    "                    if not np.array_equal(self.new_policy.get(x,y), self.start_policy.get(x,y)):\n",
    "                        optimal = False   # current policy is not optimal if a change has just been calculated\n",
    "                        \n",
    "        return optimal, self.new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# --- Policy Improvement (testing one step) ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# policy = Policy() # random policy to start\n",
    "\n",
    "# ipe = IterativePolicyEvaluation(policy)\n",
    "# value_function, _ = ipe.evaluation_step()   # calculate value function for random policy\n",
    "\n",
    "# print(f\"Start :\")\n",
    "# print(f\"Policy = \")\n",
    "# print(policy.get_graphic_display())\n",
    "# print(f\"Value function :\")\n",
    "# value_function.display()\n",
    "\n",
    "# policy_improvement = PolicyImprovement(policy, value_function)  # starting point : random policy with associated value function\n",
    "# optimal, new_policy = policy_improvement.improvement_step()\n",
    "\n",
    "# print(f\"Stop :\")\n",
    "# print(f\"Policy = \")\n",
    "# print(new_policy.get_graphic_display())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point de départ :\n",
      "Policy\n",
      "[['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)'\n",
      "  'U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)']\n",
      " ['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)'\n",
      "  'U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)']\n",
      " ['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)'\n",
      "  'U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)']\n",
      " ['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)'\n",
      "  'U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)']]\n",
      "VF evaluation (one step):\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "Iteration 1\n",
      "Optimality reached : False\n",
      "New Policy:\n",
      "[['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.00)D(0.00)R(0.00)L(1.00)'\n",
      "  'U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)']\n",
      " ['U(1.00)D(0.00)R(0.00)L(0.00)' 'U(0.25)D(0.25)R(0.25)L(0.25)'\n",
      "  'U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)']\n",
      " ['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)'\n",
      "  'U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.00)D(1.00)R(0.00)L(0.00)']\n",
      " ['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.25)D(0.25)R(0.25)L(0.25)'\n",
      "  'U(0.00)D(0.00)R(1.00)L(0.00)' 'U(0.25)D(0.25)R(0.25)L(0.25)']]\n",
      "New value function :\n",
      "[[ 0. -1. -7. -9.]\n",
      " [-1. -5. -7. -7.]\n",
      " [-7. -7. -5. -1.]\n",
      " [-9. -7. -1.  0.]]\n",
      "Iteration 2\n",
      "Optimality reached : False\n",
      "New Policy:\n",
      "[['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.00)D(0.00)R(0.00)L(1.00)'\n",
      "  'U(0.00)D(0.00)R(0.00)L(1.00)' 'U(0.00)D(0.50)R(0.00)L(0.50)']\n",
      " ['U(1.00)D(0.00)R(0.00)L(0.00)' 'U(0.50)D(0.00)R(0.00)L(0.50)'\n",
      "  'U(0.00)D(0.50)R(0.00)L(0.50)' 'U(0.00)D(1.00)R(0.00)L(0.00)']\n",
      " ['U(1.00)D(0.00)R(0.00)L(0.00)' 'U(0.50)D(0.00)R(0.50)L(0.00)'\n",
      "  'U(0.00)D(0.50)R(0.50)L(0.00)' 'U(0.00)D(1.00)R(0.00)L(0.00)']\n",
      " ['U(0.50)D(0.00)R(0.50)L(0.00)' 'U(0.00)D(0.00)R(1.00)L(0.00)'\n",
      "  'U(0.00)D(0.00)R(1.00)L(0.00)' 'U(0.25)D(0.25)R(0.25)L(0.25)']]\n",
      "New value function :\n",
      "[[ 0. -1. -7. -9.]\n",
      " [-1. -5. -7. -7.]\n",
      " [-7. -7. -5. -1.]\n",
      " [-9. -7. -1.  0.]]\n",
      "Iteration 3\n",
      "Optimality reached : True\n",
      "New Policy:\n",
      "[['U(0.25)D(0.25)R(0.25)L(0.25)' 'U(0.00)D(0.00)R(0.00)L(1.00)'\n",
      "  'U(0.00)D(0.00)R(0.00)L(1.00)' 'U(0.00)D(0.50)R(0.00)L(0.50)']\n",
      " ['U(1.00)D(0.00)R(0.00)L(0.00)' 'U(0.50)D(0.00)R(0.00)L(0.50)'\n",
      "  'U(0.00)D(0.50)R(0.00)L(0.50)' 'U(0.00)D(1.00)R(0.00)L(0.00)']\n",
      " ['U(1.00)D(0.00)R(0.00)L(0.00)' 'U(0.50)D(0.00)R(0.50)L(0.00)'\n",
      "  'U(0.00)D(0.50)R(0.50)L(0.00)' 'U(0.00)D(1.00)R(0.00)L(0.00)']\n",
      " ['U(0.50)D(0.00)R(0.50)L(0.00)' 'U(0.00)D(0.00)R(1.00)L(0.00)'\n",
      "  'U(0.00)D(0.00)R(1.00)L(0.00)' 'U(0.25)D(0.25)R(0.25)L(0.25)']]\n",
      "New value function :\n",
      "[[ 0. -1. -7. -9.]\n",
      " [-1. -5. -7. -7.]\n",
      " [-7. -7. -5. -1.]\n",
      " [-9. -7. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "# --- Policy Iteration --------------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "old_policy = Policy()   # random policy to start\n",
    "ipe = IterativePolicyEvaluation(old_policy)\n",
    "old_value_function, delta_vf = ipe.evaluation_step()\n",
    "\n",
    "print(f\"Point de départ :\")\n",
    "print(\"Policy\")\n",
    "print(old_policy.get_graphic_display())\n",
    "print(f\"VF evaluation (one step):\")\n",
    "old_value_function.display()\n",
    "\n",
    "iter_ctr = 0\n",
    "optimal = False\n",
    "\n",
    "THETA = 1e-12\n",
    "delta_vf = 2 * THETA\n",
    "\n",
    "while optimal is False:\n",
    "    iter_ctr += 1\n",
    "    print(f\"Iteration {iter_ctr}\")\n",
    "    \n",
    "    # improve policy based on last calculated value function\n",
    "    policy_improvement = PolicyImprovement(old_policy, old_value_function)\n",
    "    optimal, new_policy = policy_improvement.improvement_step()\n",
    "    \n",
    "    # iterative policy evaluation of the new policy : gives new value function\n",
    "    iter_counter_pe = 0\n",
    "    ipe = IterativePolicyEvaluation(policy=new_policy, vf_old=old_value_function)\n",
    "    while delta_vf > THETA:\n",
    "        iter_counter_pe += 1\n",
    "        vf_evaluation, delta_vf = ipe.evaluation_step()\n",
    "        # if iter_counter % 100 == 0:\n",
    "        #     print(f\"Iteration {iter_counter}\")\n",
    "        #     print(f\"Value Function après calcul :\")\n",
    "        #     vf_evaluation.display()\n",
    "        #     print(f\"Norme 2 = {delta_vf:.7f}\")\n",
    "        ipe.vf_old = vf_evaluation\n",
    "        ipe.vf_new = ValueFunction()\n",
    "    new_value_function = ipe.vf_old\n",
    "    \n",
    "    # donne des nouvelles\n",
    "    print(f\"Optimality reached : {optimal}\")\n",
    "    print(f\"New Policy:\")\n",
    "    print(new_policy.get_graphic_display())\n",
    "    print(f\"New value function :\")\n",
    "    new_value_function.display()\n",
    "    # updates\n",
    "    old_value_function = new_value_function\n",
    "    old_policy = new_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
