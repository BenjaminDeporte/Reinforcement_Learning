{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jack's Car Rental toy case\n",
    "\n",
    "From Sutton and Barto's \"Reinforcement Learning\" ch4 p81\n",
    "\n",
    "Jack owns a car rental company, with two locations (1 and 2), each hosting a given maximum number of cars.\n",
    "Customers show up at each location, and return cars, daily, according to Poisson's laws. If a car is available when a customer shows up, Jack gets business revenue.\n",
    "Jack can transfer cars overnight from one location to the other, up to a maximum number of transferts. Each transfer costs money.\n",
    "\n",
    "What is the best policy, ie given the number of cars at each location at the end of the day, how many should be transfered to maximize business the following day ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- librairies ---------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- parameters ---------------------------------------------------\n",
    "\n",
    "MAX_CARS = 4 # maximum number of cars at each rental location\n",
    "MAX_TRANSFERTS = 3  # maximum number of cars that can be moved overnight\n",
    "GAMMA = 0.9 # discount\n",
    "LAMBDA_CUSTOMERS_1 = 3  # Poisson law parameter for customer requests at location 1\n",
    "LAMBDA_CUSTOMERS_2 = 4  # Poisson law parameter for customer requests at location 2\n",
    "LAMBDA_RETURNS_1 = 3 # Poisson law parameter for cars returns at location 1\n",
    "LAMBDA_RETURNS_2 = 2 # Poisson law parameter for cars returns at location 1\n",
    "UNITARY_TRANSFERT_COST = 2  # cost of moving one car overnight\n",
    "UNITARY_RENTAL_PRICE = 10  # revenue for renting one car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Poisson laws : calculate log probas from 0 to N_CARS included -------\n",
    "#--- NB : probas for n > N_CARS are assumed negligible -------------------\n",
    "\n",
    "customers_1 = np.array([ n*np.log(LAMBDA_CUSTOMERS_1) - np.log(math.factorial(n))-LAMBDA_CUSTOMERS_1 for n in range(MAX_CARS+1)])\n",
    "customers_2 = np.array([ n*np.log(LAMBDA_CUSTOMERS_2) - np.log(math.factorial(n))-LAMBDA_CUSTOMERS_2 for n in range(MAX_CARS+1)])\n",
    "returns_1 = np.array([ n*np.log(LAMBDA_RETURNS_1) - np.log(math.factorial(n))-LAMBDA_RETURNS_1 for n in range(MAX_CARS+1)])\n",
    "returns_2 = np.array([ n*np.log(LAMBDA_RETURNS_2) - np.log(math.factorial(n))-LAMBDA_RETURNS_2 for n in range(MAX_CARS+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "#--- transition logic given state, action and daily business and returns ----------------\n",
    "#----------------------------------------------------------------------------------------\n",
    "    \n",
    "def transition(state, action, daily_numbers):\n",
    "    \"\"\"Basic transition logic. \n",
    "    Takes a state (n1, n2) at end of business day, an action (number of cars to transfer overnight), and a set of daily events (cars requests and returns during the day).\n",
    "    Calculate the end state (n1,n2) at enf of next day, and the reward\n",
    "\n",
    "    Args:\n",
    "        state (np.array(2,1)): state at end of business day, ie [n1,n2] with n1 number of cars at location 1, n2 number of cars at location 2.\n",
    "        action (int): action. number of cars to transfer from location 1 to location 2. Must be between -MAX_TRANSFERTS and +MAX_TRANSFERTS\n",
    "        daily_numbers (np.array(4,1))): array (B1,B2,R1,R2) with : B1 number of cars requests at location 1, B2 number of cars requests at location 2, R1 number of returns at location 1, R2 number of returns at location 2.\n",
    "        \n",
    "    Returns:\n",
    "        new_state (np.array(2,1)) : state after tranferts and processing of daily business\n",
    "        reward : business return of the day\n",
    "    \"\"\"\n",
    "    \n",
    "    # init reward\n",
    "    reward = 0\n",
    "    # get numbers of cars end of previous day\n",
    "    n1_t = state[0]\n",
    "    n2_t = state[1]\n",
    "    # execute overnight transfer\n",
    "    if action > n1_t:\n",
    "        raise NameError(f\"Location 1 has {n1_t} cars but {action} are requested to transfer\")\n",
    "    if action < -n2_t:\n",
    "        raise NameError(f\"Location 2 has {n2_t} cars but {action} are requested to transfer\")\n",
    "    n1 = n1_t - action\n",
    "    n2 = n2_t + action\n",
    "    reward -= np.abs(action) * UNITARY_TRANSFERT_COST\n",
    "    # calculate number of cars being rented during day\n",
    "    B1 = daily_numbers[0]\n",
    "    rent1 = min(n1, B1) # can not rent more than the stock\n",
    "    B2 = daily_numbers[1]\n",
    "    rent2 = min(n2, B2) # can not rent more than the stock\n",
    "    reward += (rent1 + rent2) * UNITARY_RENTAL_PRICE\n",
    "    # get returns during the day\n",
    "    R1 = daily_numbers[2]\n",
    "    R2 = daily_numbers[3]\n",
    "    # calculate stocks end of current day, capped at MAX_CARS\n",
    "    # cast as int because used as indexes for array\n",
    "    n1_t_plus_1 = int(min( n1 - rent1 + R1, MAX_CARS))\n",
    "    n2_t_plus_1 = int(min( n2 - rent2 + R2, MAX_CARS))\n",
    "    # format outputs\n",
    "    new_state = np.array([n1_t_plus_1, n2_t_plus_1])\n",
    "    \n",
    "    return new_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy class\n",
    "\n",
    "Hosts policy evaluation, policy improvement algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "#--- classe pour Policy ---------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "class DeterministicPolicy():\n",
    "    \"\"\"This is the class to manage a deterministic policy:\n",
    "    - holds records of every action (ie number of cars to transfer overnight) per given state\n",
    "    - sanitization to ensure tranfers are possible (ie do not go beyond numbers of cars)\n",
    "    - holds the iterative algorithm to converge to the policy value function\n",
    "    \"\"\"\n",
    "    \n",
    "    THETA = 1e-6   # convergence criterion\n",
    "    IMPROVEMENT_THRESHOLD = 1e-9  # check improvement in two successive value functions\n",
    "    \n",
    "    # --- constructor ------------------------------------------------------------------------\n",
    "    def __init__(self, actions_array=None):\n",
    "        \n",
    "        # the policy is an array of MAX_CARS x MAX_CARS of number of cars to transfer overnight from location 1 to location 2\n",
    "        if actions_array is None:\n",
    "            # if no policy is given, init to 0 (no cars transfered)\n",
    "            self._actions = np.zeros((MAX_CARS+1, MAX_CARS+1))\n",
    "        else:\n",
    "            # if a policy is given,\n",
    "            # check shape\n",
    "            assert actions_array.shape == (MAX_CARS+1, MAX_CARS+1), \"Wrong policy shape passed to DeterministicPolicy constructor\"\n",
    "            # store the policy\n",
    "            self._actions = actions_array\n",
    "            # sanitize policy\n",
    "            sanitized = self._sanitize_actions()\n",
    "            if sanitized:\n",
    "                print(f\"action array got clipped in DeterministicPolicy constructor\")\n",
    "\n",
    "        # place holder for policy value funtion (to calculate)\n",
    "        self._policy_value_function = None\n",
    "        \n",
    "        # value functions arrayS for the iterative calculation\n",
    "        self._old_vf = np.zeros((MAX_CARS+1, MAX_CARS+1))\n",
    "        self._new_vf = np.zeros((MAX_CARS+1, MAX_CARS+1))\n",
    "        \n",
    "    # --- info ----\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Object DeterministicPolicy, action array shape = {self.actions.shape}, policy evaluated : {self._policy_value_function is not None}\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Object DeterministicPolicy, action array shape = {self.actions.shape}, policy evaluated : {self._policy_value_function is not None}\"\n",
    "\n",
    "    # --- sanitizor -------------------------------------------------------------------------\n",
    "    def _sanitize_actions(self):\n",
    "        # check every action is possible given locations' cars stocks\n",
    "        sanitized = False\n",
    "        for n1 in range(MAX_CARS+1):\n",
    "            for n2 in range(MAX_CARS+1):\n",
    "                # get projected number of cars to transfer from location 1 with n1 to location 2 with n2\n",
    "                current_action = self._actions[n1,n2]\n",
    "                # check transfert is possible, if not, clip number\n",
    "                if current_action > n1 :\n",
    "                    self._actions[n1,n2] = n1\n",
    "                    sanitized = True\n",
    "                if current_action < -n2:\n",
    "                    self._actions[n1,n2] = -n2\n",
    "                    sanitized = True\n",
    "        return sanitized\n",
    "    \n",
    "    # --- get, set functions --------------------------------------------------------------------------\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return self._actions\n",
    "    \n",
    "    @actions.setter\n",
    "    def actions(self, x):\n",
    "        assert x.shape == (MAX_CARS+1, MAX_CARS+1), \"Wrong policy shape passed to actions setter in DeterministicPolicy object\"\n",
    "        self._actions = x\n",
    "        sanitized = self._sanitize_actions()\n",
    "        if sanitized:\n",
    "            print(f\"action array got clipped when used to set a DeterministicPolicy\")\n",
    "    \n",
    "    @property\n",
    "    def policy_value_function(self):\n",
    "        # if self._policy_value_function is None:\n",
    "        self._policy_evaluation()\n",
    "            \n",
    "        return self._policy_value_function\n",
    "    \n",
    "    @policy_value_function.setter\n",
    "    def policy_value_function(self):\n",
    "        raise NameError(f\"Attempt to write a policy evaluation directly in a DeterministicPolicy object\")\n",
    "    \n",
    "    # @policy_value_function.setter\n",
    "    # def policy_value_function(self):\n",
    "    #     raise NameError(f\"Attempt to write directly a value function in DeterministicPolicy object\")\n",
    "    \n",
    "    # --- one policy evaluation step ------------------------------------------------------------------\n",
    "    \n",
    "    def _evaluation_step(self):\n",
    "        \"\"\"perform one step of policy evaluation, update self._new_vf\n",
    "        \"\"\"\n",
    "        \n",
    "        # init\n",
    "        old_vf = self._old_vf\n",
    "        new_vf = np.zeros_like(old_vf)\n",
    "        \n",
    "        # check feasibility of policy for every state\n",
    "        sanitized = self._sanitize_actions()\n",
    "        if sanitized is True:\n",
    "            print(f\"Policy was sanitized (ie some tranferts were clipped) prior to policy evaluation\")\n",
    "        \n",
    "        # number of sweeps\n",
    "        number_sweeps_to_perform = (MAX_CARS+1)**6\n",
    "        number_sweeps_performed = 0\n",
    "        # perform ONE sweep\n",
    "        for n1 in range(MAX_CARS+1):\n",
    "            for n2 in range(MAX_CARS+1):\n",
    "                # get starting state\n",
    "                state = np.array([n1,n2])\n",
    "                # get the policy action planned for state (n1,n2), feasibility has been checked above\n",
    "                policy_action = self.actions[n1,n2]\n",
    "                # envision all possible business events -----------------------------------------------\n",
    "                # we consider only business rentals requests up to MAX_CARS\n",
    "                for B1 in range(MAX_CARS+1):\n",
    "                    # get log proba of having B1 requets according to the Poisson law\n",
    "                    log_pB1 = customers_1[B1] \n",
    "                    for B2 in range(MAX_CARS+1):\n",
    "                        log_pB2 = customers_2[B2]  \n",
    "                        # consider only returns up to MAX_CARS\n",
    "                        for R1 in range(MAX_CARS+1):  \n",
    "                            log_pR1 = returns_1[R1] \n",
    "                            for R2 in range(MAX_CARS+1):\n",
    "                                log_pR2 = returns_2[R2]\n",
    "                                # calculate total probability of all four events, assumed independent of course\n",
    "                                log_p = log_pB1 + log_pB2 + log_pR1 + log_pR2\n",
    "                                # calculate end state\n",
    "                                daily_numbers = np.array([B1,B2,R1,R2])\n",
    "                                new_state, reward = transition(state, policy_action, daily_numbers)\n",
    "                                # calculate delta for value function\n",
    "                                delta_vf = np.exp(log_p) * ( reward + GAMMA * old_vf[new_state[0], new_state[1]])\n",
    "                                # update value function\n",
    "                                new_vf[n1,n2] = new_vf[n1,n2] + delta_vf\n",
    "                                # update\n",
    "                                number_sweeps_performed += 1\n",
    "                                # print(f\"calculated {number_sweeps_performed} expected returns / {number_sweeps_to_perform}\", end=\"\\r\")\n",
    "                                \n",
    "        # at this point, one sweep has been performed and the next iteration of value function wrt old_vf has been computed in new_vf\n",
    "        self._new_vf = new_vf\n",
    "        \n",
    "    # --- full policy evaluation ----------------------------------------------------------------------\n",
    "    \n",
    "    def _policy_evaluation(self):\n",
    "        \"\"\"Evaluate policy. Iterations until convergence\n",
    "        \"\"\"\n",
    "        \n",
    "        convergence_criterion = 2 * self.THETA\n",
    "        \n",
    "        # inits\n",
    "        # start from 0 policy (no transferts)\n",
    "        self._old_vf = np.zeros((MAX_CARS+1, MAX_CARS+1))\n",
    "        # counting\n",
    "        iteration_number = 1\n",
    "        # print(f\"starting policy evaluation\")\n",
    "        # loop\n",
    "        while convergence_criterion > self.THETA:\n",
    "            # print(f\"iteration number : {iteration_number} -----------------------------------\")\n",
    "            # perform one step\n",
    "            self._evaluation_step()\n",
    "            convergence_criterion = np.max(np.abs(self._old_vf - self._new_vf))\n",
    "            self._old_vf = self._new_vf\n",
    "            iteration_number += 1\n",
    "            print(f\"Iteration {iteration_number} - Norm inf convergence criterion = {convergence_criterion:.2e}\", end=\"\\r\")\n",
    "        # iteration is complete\n",
    "        print()\n",
    "        self._new_vf = self._old_vf\n",
    "        self._policy_value_function = self._new_vf\n",
    "        \n",
    "    # --- policy improvement --------------------------------------------------------------------------\n",
    "    \n",
    "    def _policy_improvement(self):\n",
    "        \"\"\"Perform a one-step policy improvement of a current policy with an associated value function\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the value function of the policy (NB : assumed to be calculated already)\n",
    "        pvf = self._policy_value_function\n",
    "        \n",
    "        # inits\n",
    "        # get the current policy (ie action per state) and place holder for improved policy\n",
    "        # check feasibility of policy for every state\n",
    "        sanitized = self._sanitize_actions()\n",
    "        if sanitized is True:\n",
    "            print(f\"Policy was sanitized (ie some tranferts were clipped) prior to policy improvement\")\n",
    "        old_policy = self.actions\n",
    "        new_policy = np.zeros((MAX_CARS+1, MAX_CARS+1))\n",
    "        \n",
    "        # change flag\n",
    "        optimized = False\n",
    "        \n",
    "        # loop\n",
    "        # number of sweeps\n",
    "        number_sweeps_to_perform = (MAX_CARS+1)**6 * (2*MAX_TRANSFERTS+1)\n",
    "        number_sweeps_performed = 0\n",
    "        # perform ONE sweep\n",
    "        for n1 in range(MAX_CARS+1):\n",
    "            for n2 in range(MAX_CARS+1):\n",
    "                # get starting state\n",
    "                state = np.array([n1,n2])\n",
    "                # get current value function of the state and current action\n",
    "                current_vf = pvf[n1,n2]\n",
    "                current_action = old_policy[n1,n2]\n",
    "                # try all actions and calculate their q values\n",
    "                q_values = np.zeros(2*MAX_TRANSFERTS+1)\n",
    "                for action in range(-MAX_TRANSFERTS, +MAX_TRANSFERTS+1):\n",
    "                    # skip impossible actions\n",
    "                    if action > n1: \n",
    "                        number_sweeps_performed += (MAX_CARS+1)**4\n",
    "                        continue\n",
    "                    if action < -n2: \n",
    "                        number_sweeps_performed += (MAX_CARS+1)**4\n",
    "                        continue\n",
    "                    # calculate q_value of action considered\n",
    "                    q_value = 0\n",
    "                    # we consider only business rentals requests up to MAX_CARS\n",
    "                    for B1 in range(MAX_CARS+1):\n",
    "                        # get log proba of having B1 requets according to the Poisson law\n",
    "                        log_pB1 = customers_1[B1] \n",
    "                        for B2 in range(MAX_CARS+1):\n",
    "                            log_pB2 = customers_2[B2]  \n",
    "                            # consider only returns up to MAX_CARS\n",
    "                            for R1 in range(MAX_CARS+1):  \n",
    "                                log_pR1 = returns_1[R1] \n",
    "                                for R2 in range(MAX_CARS+1):\n",
    "                                    log_pR2 = returns_2[R2]\n",
    "                                    # calculate total probability of all four events, assumed independent of course\n",
    "                                    log_p = log_pB1 + log_pB2 + log_pR1 + log_pR2\n",
    "                                    # calculate end state\n",
    "                                    daily_numbers = np.array([B1,B2,R1,R2])\n",
    "                                    new_state, reward = transition(state, action, daily_numbers)\n",
    "                                    # get q value for starting state and envisoned action\n",
    "                                    q_value += np.exp(log_p) * ( reward + GAMMA * pvf[new_state[0], new_state[1]] )\n",
    "                                    # update\n",
    "                                    number_sweeps_performed += 1\n",
    "                                    print(f\"calculated {number_sweeps_performed} situations / {number_sweeps_to_perform}\", end=\"\\r\")\n",
    "                    q_values[action+MAX_TRANSFERTS] = q_value\n",
    "                # find argmax q_values and check if better\n",
    "                max_q_value = np.max(q_values)\n",
    "                if max_q_value > current_vf + self.IMPROVEMENT_THRESHOLD:  \n",
    "                    # yes, there is a q_value better than the current value_function : improve policy !\n",
    "                    id_argmax = np.argmax(q_values)\n",
    "                    action_max = id_argmax - MAX_TRANSFERTS\n",
    "                    new_policy[n1,n2] = action_max\n",
    "                    # signal that policy has been strictly improved\n",
    "                    optimized = True\n",
    "        print()\n",
    "        \n",
    "        # calculate infinite norm between the old and new value functions\n",
    "        gain = np.max(np.abs(new_policy - old_policy))\n",
    "        \n",
    "        # policy is considered improved if it has changed AND value function has increased above a threshold                                    \n",
    "        if (optimized is True and gain > self.IMPROVEMENT_THRESHOLD):\n",
    "            # print(f\"\\nPolicy has improved\")\n",
    "            self.actions = new_policy\n",
    "        else:\n",
    "            optimized = False\n",
    "            # print(f\"\\nPolicy is optimal\")\n",
    "\n",
    "        return optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Policy Iteration loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dp = DeterministicPolicy()\n",
    "\n",
    "optimized = False\n",
    "iter = 1\n",
    "\n",
    "print(f\"--- Policy Iteration for Jack's Car Rental --------\")\n",
    "print(f\"Maximum number of cars at each location : {MAX_CARS}\")\n",
    "print(f\"Maximum number of transferts overnight : {MAX_TRANSFERTS}\")\n",
    "print()\n",
    "\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    while True:\n",
    "        print(f\"\\nIteration {iter}\")\n",
    "        print(\"Current policy is:\")\n",
    "        print(dp.actions)\n",
    "        print(f\"Evaluating current policy (ie calculate policy's value function)...\")\n",
    "        print(dp.policy_value_function)\n",
    "        print(f\"... value function calculated\")\n",
    "        print(f\"Try to improve policy...\")\n",
    "        optimized = dp._policy_improvement()\n",
    "        if optimized is False:\n",
    "            print(f\"...Current policy is optimal\")\n",
    "            break\n",
    "        print(f\"... calculated a better policy\")\n",
    "        iter += 1\n",
    "\n",
    "    print(f\"\\nEnd of iterations\")\n",
    "    print(f\"Optimal policy found\")\n",
    "    print(dp.actions)\n",
    "    print(f\"Value function:\")\n",
    "    print(dp.policy_value_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
